# 一致性

[多视角](%E4%B8%80%E8%87%B4%E6%80%A7/%E5%A4%9A%E8%A7%86%E8%A7%92.md)

ConsisID：开源的角色一致性文生视频模型
项目主页: [https://pku-yuangroup.github.io/ConsisID/](https://pku-yuangroup.github.io/ConsisID/)
在线试用: [https://huggingface.co/spaces/BestWishYsh/ConsisID-preview-Space](https://huggingface.co/spaces/BestWishYsh/ConsisID-preview-Space)

工作流：[工作流演示视频](https://www.bilibili.com/video/BV1cX4y1z7Cb)

1、使用 PR 自动重构为竖屏视频，然后用 ffmpeg 将视频转为帧率 18 的图片序列

2、使用 [Grounding DINO + Segment Anything](https://github.com/continue-revolution/sd-webui-segment-anything) 将人物主体切分出来。这其中发现用 girl 作为 prompt 进行分割会偶尔导致大幅动作时双马尾脱锁，因此用 twin tails 又跑了一边，然后将 mask 合并

3、使用 [WD 1.4 tagger](https://github.com/toriato/stable-diffusion-webui-wd14-tagger) 提取各帧提示词（阈值 0.65），然后用[数据集标签编辑器](https://github.com/toshiaki1729/stable-diffusion-webui-dataset-tag-editor)进行批量编辑，主要编辑点：

- 误识别的 thighhighs, black thighhighs，修正为 thigh boots, black thigh boots
- 去除 panties, underwear, pantyshot 等词条
- 添加 smile，不然一副面无表情的样子
- 添加 black background, simple background

4、更新了[多帧渲染插件](https://github.com/OedoSoldier/sd-webui-image-sequence-toolkit)，现在多帧渲染和之前的 enhanced img2img 都支持了 ControlNet 1.1 的 inpaint 模型

5、具体参数：

- 模型：[Aniflatmix](https://civitai.com/models/24387)（[HuggingFace镜像](https://huggingface.co/OedoSoldier/aniflatmix)）
- 默认提示词：masterpiece, best quality, anime screencap
- 使用的负面 embed：[EasyNegative](https://civitai.com/models/7808), [badhandv4](https://civitai.com/models/16993), [verybadimagenegative_v1.3](https://civitai.com/models/11772)
- 生成分辨率：768 * 1360
- CFG Scale：4
- 重绘幅度：0.75
- ControlNet：全部启用 pixel perfect 和无提示词模式，权重均为 1，画面缩放模式均为"仅调整大小（拉伸）"
- inpaint（预处理器：inpaint global harmonious）
- ip2p（预处理器：none）
- shuffle（预处理器：none）
- lineart anime（预处理器：lineart anime）
- softedge（预处理器：softedge pininet）
- 多帧渲染 initial denoising strength 设置为 0.75，启用 ControlNet inpaint，并从文件读取提示词，其他参数默认

6、修正部分作画错误，用 PR 合成视频

顺带吐槽本来是昨晚睡前开始跑，结果跑到一半不知道什么原因报错，早上起来才发现只跑了两百帧，不然早就跑完了，幸好我的脚本支持继续任务，不然又得多跑好久。算下来七百多帧大约跑了八个多小时。